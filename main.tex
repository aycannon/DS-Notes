\documentclass[11pt]{article}
\input{preamble}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
\title{Data Science for Economics}

\thispagestyle{empty}

\begin{center}
{\LARGE \bf Data Science for Economics}\\
{\large Archie Cannon}\\
\today
\end{center}
% use \begin{shaded} and \begin{note}

% Contents (all formatting here, other than colours)
{
\begin{tcolorbox}[title=Contents, fonttitle=\huge\sffamily\bfseries\selectfont,interior style={left color=contcol1!50!white,right color=contcol2!50!white},frame style={left color=contcol1!80!white,right color=contcol2!80!white},coltitle=black,top=2mm,bottom=2mm,left=2mm,right=2mm,drop fuzzy shadow,enhanced,breakable]
\makeatletter
\@starttoc{toc}
\makeatother
\end{tcolorbox}}

\newpage

\section{Regression}

We want to predict the response $y$ from inputs $\mathbf{x} = (1, x_1, x_2, x_3, \ldots, x_p)$. We can do this by modelling the \hl{conditional mean of $y$ conditional on $\mathbf{x}$}

\begin{equation*}
    \E(y|\mathbf{x}) = f(\mathbf{x^\prime\beta}) = (\beta_0 + \beta_1 x_1 + \beta_2 + x_2 + \ldots + \beta_p x_p)
\end{equation*}
$f$ is a link function - linking the explanatory variable matrix and the coefficient vector. $\epsilon \equiv y - \E(y|\mathbf{x})$ is the error term. We can rewrite the model as a combination of the conditional mean and the error:

\begin{equation}
    y = \E(y|\mathbf{x}) + \epsilon
\end{equation}

\begin{note}
    $\E(\epsilon|\mathbf{x}) = 0$
\end{note}

\begin{note}
    \hl{Unconditional means are just a number. Conditional means are a function that depends on $\mathbf{x}$}.
\end{note}

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{pic/conditional mean.png}
    \caption{Difference between conditional and unconditional means}
    \label{fig:conditional means}
\end{figure}

\subsection{Linear Regression}

Linear regressions have the link function $f(z) = z$. This gives us:

\begin{equation*}
    \E(y|\mathbf{x}) = f(\mathbf{x^\prime\beta}) = (\beta_0 + \beta_1 x_1 + \beta_2 x_2 +\ldots + \beta_p x_p)
\end{equation*}

Let $\epsilon \equiv y - \mathbf{x^\prime\beta}$ is the error term and $\E[\epsilon|\mathbf{x}] = 0$, this gives us:

\begin{equation}
    \label{eq:lin reg}
    y = \mathbf{x^\prime\beta} + \epsilon   
\end{equation}

We often assume that the errors are normally distributed, i.e. $\epsilon \sim \mathcal{N}(0,\sigma^2)$, giving:

\begin{equation}
\label{gaussian linear reg}
    y \sim \mathcal{N}(\mathbf{x^\prime\beta}, \sigma^2) = \dfrac{1}{\sqrt{2\pi\sigma^2}}\exp\left[-\dfrac{(y - \mathbf{x^\prime\beta})^2}{2\sigma^2}\right]
\end{equation}

\subsection{Log Linearity}

We may want to model the conditional mean of $\ln y$ rather than $y$ depending on the relationship between the outcome and regressors. This means that the relationship between the variables is multiplicative rather than additive.

\begin{equation*}
    \ln y = \alpha + \beta x + \epsilon \Leftrightarrow y = e^\alpha e^{\beta x}e^\epsilon
\end{equation*}

How does a unit change in $x$ affect $y$:$\beta \approx \Delta y /y$, meaning $\beta$ measures the percentage change in $y$ following a unit change in $x$

\begin{note}
    We use $\ln y$ whenever $y$ changes on a \hl{percentage scale or is strictly positive}. 
\end{note}

\subsection{Log-Log}
If we want to model the elasticity of variables we use log-log-models. That means we log the RHS as well. i.e.

\begin{equation*}
    \ln y = \beta_0 + \beta_1 \ln x + \epsilon
\end{equation*}

This means that $\beta_1$ can be interpreted as 
\begin{equation*}
    \beta_1 \approx \dfrac{\Delta y / y}{\Delta x/ x}
\end{equation*}

This means that we interpret this as a 1\% change in $x$ leads to a $\beta_1 \%$ change in $y$.

\subsection{Dummy Variables}

We will look at the example of orange juice sales. Specifically using brand dummy variables:

\begin{equation*}
\text{brandtropicana} = \begin{cases}
        1 & \text{if the obs was tropicana} \\
        0 & \text{otherwise}
    \end{cases}
\end{equation*}

We will also have a similar indicator variable for the other brand Minute Maid. The reason we do not have an indicator variable for every brand is to avoid perfect multicollinearity. \hl{The brand which we do not have an indicator variable for will be the reference variable.}

By introducing dummy variables, we will have different intercepts for the different brands. Let $x = price$

$$
\E[\ln y| \cdot] = \beta_0 + \beta_1 \ln x + \beta_2 tropicana + \beta_2 minutemaid
$$

intercept for Tropicana is now $\beta_0 + \beta_2$. We can write this model short-hand by using subscripts:

\begin{equation*}
    \E[\ln y|\cdot] = \beta_{0,b} + \beta_1 \ln x
\end{equation*}
where $b$ indicates the different brands

\subsection{Interactions}

When we want to model how one variable affects how another variable may affect our outcome $y$, we use interaction terms. This is just the product of two regressors.

\begin{equation}
    \label{eq:interaction}
    \E[y|\mathbf{x}] = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_1 x_2
\end{equation}

This means that the effect on $y$ from a unit change in $x_1$ is $\beta_1 + \beta_3 x_2$. 

\begin{note}
    Interactions can very quickly increase the number of regressors in our model
\end{note}

Back to our oj data, if we were to add interaction terms between brands and prices we would have the model:

\begin{equation*}
    \E[y|\cdot] = \beta_{0,b} + \beta_{1,b} \ln x
\end{equation*}

Notice, we have brand-specific intercepts and elasticities ($\beta$ coefficient is interpreted as elasticity because of the log-log model).

\subsection{Generalised Linear Model (GLM)}

So far, we have thought about regressions as minimising the sum of squared residuals (OLS), but we actually tend to use a more general approach of \textit{maximum likelihood}. 

The likelihood is the probability of the data given the coefficients. he maximum likelihood, found with the coefficients that give the largest likelihood.

In our Gaussian regression, the probability of a data point $i$ is:
\begin{equation*}
    y \sim \mathcal{N}(\mathbf{x^\prime\beta}, \sigma^2) = \dfrac{1}{\sqrt{2\pi\sigma^2}}\exp\left[-\dfrac{(y - \mathbf{x^\prime\beta})^2}{2\sigma^2}\right]
\end{equation*}

That means, because the data is i.i.d, the probability of observing the whole data set $(y,\mathbf{x})_{i=1}^n$ is

\begin{equation}
\label{eq:likelihood normal}
\mathcal{L(\mathbf{\beta}}) \equiv \prod_{i=1}^n \dfrac{1}{\sqrt{2\pi\sigma^2}}\exp\left[-\dfrac{(y - \mathbf{x^\prime\beta})^2}{2\sigma^2}\right]
\end{equation}

where $\mathcal{L(\beta)}$ is the likelihood. The maximum likelihood estimates $\beta$ such that $\mathcal{L(\beta)}$ is maximised.

\subsubsection{Deviance}

Deviance compares the likelihood of values $L$ of two model formulations. very similar to the Residual Sum of Squares.
\begin{enumerate}
    \item Saturated (a.k.a Satiated) model: Enough parameters to perfectly fit the data, $L_s$
    \item Fitted Model: The model we specify, in between the null model (no parameters) and the saturated model.
\end{enumerate}

Deviance $D$ measures the distance between the data and the fitted model by likelihoods:

\begin{equation}
    \label{eq: dev}
    D(\bm{\beta}) \equiv - 2[\ln L(\bm{\beta}) - \ln L_s]
\end{equation}

\begin{note}
    Deviance is complementary to likelihood: \hl{minimise deviance $\Leftrightarrow$ maximise likelihood}
\end{note}

\begin{note}
    We talk in terms of deviance because it applies to nonlinear models as well as linear ones. It makes the comparison between different model specifications easier to interpret.
\end{note}

\subsubsection{Log-Likelihood}

The saturated model has the perfect fit, meaning the log-likelihood is equal to:

\begin{equation*}
    \ln L(\bm{\beta}) = -\dfrac{n}{2}\ln(2\pi\sigma^2)
\end{equation*}

The log-likelihood of the fitted model is:
\begin{equation}
    \label{eq: log likelihood fitted}
    \ln L(\bm{\beta}) = -\dfrac{n}{2}\ln(2\pi\sigma^2) - \dfrac{1}{2\sigma^2}\sum_{i=1}^n(y - \mathbf{x_i^\prime}\bm{\beta})^2
\end{equation}

Together we can find the deviance:

\begin{equation}
    D(\bm{\beta}) = \dfrac{1}{\sigma^2}\underbrace{\sum_{i=1}^n (y_i - \mathbf{x_i^\prime}\bm{\beta})^2}_{\text{Sum of squared errors}}
\end{equation}

When we write it this way we can see that the deviance is proportional to the sum of squared errors, meaning that the \hl{deviance minimising estimate for $\beta$ is also the least squares estimate and also the maximum likelihood estimate.}

\paragraph{In R}: we have regression \lstinline{summary} statistics that are important here.
\begin{enumerate}
    \item \hl{Residual Deviance}, $D$, is what we are minimising:
    \begin{equation*}
        D = \sum_{i=1}^n(y_i - \mathbf{x_i^\prime}\bm{\beta})^2
    \end{equation*}
    Also known as the \textbf{residual sum of squares}
    \item \hl{Null Deviance}, $D_0$, the deviance for a model with only a constant. In the linear regression case:
    \begin{equation*}
        D_0 = \sum_{i=1}^n (y_i - \Bar{y})^2
    \end{equation*}
    Also known as the \textbf{total sum of squares}
\end{enumerate}

We know that the difference between the two is from the information about $y$ coming from $\mathbf{x}$. This is familiar, this is the proportion of information (variance) accounted for by the model, $R^2$:

\begin{equation}
    \label{eq:r2}
    R^2 \equiv \dfrac{D_0 - D}{D_0} = 1 - \dfrac{D}{D_0}
\end{equation}

When we define $R^2$ in terms of deviance it is more general compared to when we learnt about it in terms of the sum of squares.

We also have the \hl{dispersion parameter}: $\sigma^2$. Since $y \sim \mathcal{N}(\mathbf{x^\prime}\bm{\beta}, \sigma^2)$, the error variance $\sigma^2$ measures variability around the mean $\mathbf{x^\prime}\bm{\beta}$:

\begin{equation*}
    \sigma^2 = Var(\epsilon) = Var(y - \mathbf{x^\prime}\bm{\beta})
\end{equation*}

\begin{note}
    Other info: \hl{Degrees of freedom} = observations - coefficients estimated; \hl{AIC} - Akaike's Information Criteria.
\end{note}

\subsection{Prediction}
\begin{note}
    Big difference between in-sample (IS) and out-of-sample (OOS) predictions. IS: how well does the estimated model fit \textit{the} data? OOS: How well does the model fit \textit{new} data?
\end{note}

A massive aspect of this course is about the \hl{avoidance of overfitting}. Predictive ability is measured OOS to avoid overfitting.


\newpage
\section{Cross Validation}

Cross-validation is a way to estimate OOS deviance OOS fit measures. We use it for model selection: among a set of models, we want to choose the one with the best OOS predictions. Model selection often involves dimension reduction.

\subsection{In-Sample fit}

We use in-sample $R^2$ to summarise a fitted model's ability to predict outcomes in training data.

\begin{equation*}
    R^2_{IS} = \dfrac{D_{IS,0} - D_{IS}}{D_{IS,0}} = 1 - \dfrac{D_{IS}}{D_{IS,0}}
\end{equation*}
where $D_{IS,0}$ is the deviance of the null model in-sample.

\begin{note}
    This is useful, but we want to know the predictive power of the model, so we need the out-of-sample $R^2$.
\end{note}

\subsection{Split-Sample Validation}

Because we need the OOS deviance for our model, we need to split the data into training (IS) and testing (OOS) data. For split-sample validation, we will randomly split our sample of $n$ observations into $n_1$ training observations and $n_2 = n-n_1$ test observations

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{pic/split sample.png}
    \caption{Splitting the sample}
    \label{fig:split sample}
\end{figure}

Either side of the data in Figure \ref{fig:split sample} can be the training/test data, but the point is that we train on a subset of the data and test it on the other.

\begin{procedure}
    Given our subsets of data, training: $(y_i, \mathbf{x_i})_{i=1}^{n_1}$, test:$(y_i, \mathbf{x_i})_{i=1}^{n_2}$ where $n_1 + n_2 = n$. 
    
    \begin{enumerate}
        \item We will fit the model on the $n_1$ observations in the training data to get fitted coefficients. 
        \item We then take those fitted coefficients to predict outcomes on the $n_2$ test observations.
        \item From this we will calculate the OOS deviance/$R^2$
    \end{enumerate}
    \begin{equation*}
        D_{OOS} = \sum_{j=1}^{n_2}(y_j - \mathbf{x^\prime_j}\bm{\hat{\beta}})^2, \qquad D_{OOS,0} = \sum_{j=1}^{n_2}(y_j - \Bar{y})^2
    \end{equation*}

    \begin{equation*}
        R^2_{OOS} = 1 - \dfrac{D_{OOS}}{D_{OOS,0}}
    \end{equation*}
\end{procedure}

We can compute the different OOS $R^2$ for many models and pick the model with the highest $R^2_{OOS}$.
\begin{note}
    It is possible to find a negative $R^2_{OOS}$. This implies that our model is worse than the null model. Meaning, our model is overfitting the data. \hl{Overfitting: fitted model reflects noise particular to the training data, but not present in the test data.}
\end{note}

\subsection{Detecting and Avoiding Issues of Overfitting}

\begin{procedure}
    \hspace{1in}
    \subsubsection*{Validate on New data}
    By evaluating the OOS predictive ability of a model, we can detect whether it is overfitting. \hl{Can be done through split-sample or $k$-fold cross validation}
    \subsubsection*{Dimension Reduction}
    Overfitting is much more likely in high-dimensional models (many regressors relative to sample size). \hl{We can do this through the exclusion of irrelevant regressors or regularisation}.
\end{procedure}

\subsection{K-fold Cross-Validation}

Split-sample validation is not very efficient because we only have two subsets of the data. This means \textbf{maximum}, we can test the model's predictive power on two sets.

$K$-fold CV is a refinement of the split-sample:

\begin{procedure}
    \begin{enumerate}
        \item Split the $n$ observations into $K$ evenly sized subsets (folds). For $k = 1,2,\ldots, K$:
        \item Fit the model on all the data but exclude fold $k$.
        \item Record the OOS $R^2$ by testing on the excluded $k$th fold.
    \end{enumerate}
\end{procedure}

The result will be $K$ OOS/IS $R^2$. We can use the distribution of these OOS $R^2$ \textit{or just the mean}, to evaluate the model's predictive power. The split can be visualised as:

\begin{figure}[h]
    \centering
    \includegraphics[width = \textwidth]{pic/kfold.png}
    \caption{Visualisation of splitting the data}
    \label{fig:kfold split}
\end{figure}

The $k$-fold estimator of $R^2$ is more accurate as it uses the data more efficiently.

\subsection{Dimension Reduction}

It is very common to have high-dimensional models (many regressors). In this case, we may have 1000's of regressors which is way too many for interpretation. The easiest way to do this, \hl{the cut model}. Here we choose variables with the lowest $p$-values. The choice of the number of variables is completely subjective.

\begin{note}
    When $p>n$ (number of regressors is larger than the number of observations) there is no longer a unique least squares coefficient estimate, but rather an infinite number of solutions. Each of these solutions will have 0 error on the training data, but typically very poor test performance. 
\end{note}

A few things we can do:
\begin{itemize}
    \item Subset selection (dimension reduction): identifying a subset of the $p$ predictors.
    \item Shrinkage (regularisation): fit a model with all $p$ predictors, irrelevant regressors' coefficients will shrink to 0. This shrinkage reduces the variance. Shrinkage can simultaneously perform variable selection (lasso).
    \item Dimension reduction: crude (as mentioned above), or PCA where we project the $p$ predictors into an $M$-dimensional space, where $M<p$.
\end{itemize}


\subsubsection{PCR}
We talk about PCA in more detail in Section \ref{sec:PCA}

PCR offers a prediction technique and reduces the risk of overfitting through dimension reduction. We have found the principal components through PCA, we could use those components for regression. 

\newpage
\section{Regularisation}

Regularization generates a manageable set of models to choose form. We penalise complexity (number of regressors) and create a list of promising candidate models. The resulting list is called a \hl{regularisation path}.

In this crude dimension reduction we mentioned earlier, we could use a decreasing threshold $p$-value to obtain a regularisation path of fitted models ranging from complex to simple. \hl{However, this \textit{multiple testing} regularisation approach is bad as we will likely discard useful information on the response variable}. This is because multicollinearity inflates $p$-values. Additionally, because our foundation is the full model, the $p$-values are obtained from an overfitted model with terrible a OOS fit. Finally if $p>n$, we cannot even fit the full model as mentioned, so we have no starting point.

\subsection{Stepwise Regression}

There is both forward and backwards stepwise regression. Backwards stepwise: starting with the full model and cut down (issues are the same as above). \hl{Forward stepwise: starting from the null}.

\begin{algo}
    \hspace{1in}
    Let $\mathcal{M}_0$ denote the \textit{null model} (0 predictors)

    For $k = 0, 1, 2, \ldots, p-1$:

    \begin{enumerate}
        \item Consider all $p-k$ models that augment the predictors in $\mathcal{M}_k$ with \textbf{one additional predictor} (add each regressor one at a time)
        \item Let the \textit{best} among the $p-k$ models be called $\mathcal{M}_{k+1}$. 
        \begin{note}
            Best is defined as having the smallest RSS or highest $R^2$
        \end{note}
        \item We now have a regularisation path $\mathcal{M}_0, \ldots, \mathcal{M}_p$. Select the best model along the regularisation path, using the OOS $R^2$.
    \end{enumerate}

    We can stop the algorithm based on whatever we want. It is common to stop either when reaching a predetermined number of regressors, or when information criteria says that additional complexity does not improve IS fit.
\end{algo}

\begin{note}
    We can select the final model through a few methods, not only the OOS $R^2$. Prediction error on a validation set, AIC, BIC, adjusted $R^2$, or \hl{cross validation method}
\end{note}

\begin{shaded}
    \textbf{Problems with forward step regression regularisation}:

    \begin{itemize}
        \item This algorithm is a \hl{greedy} algorithm. It is \textit{myopic}, each iteration it makes the loacl best choice, which may not be the globally best
        \item The process is very \hl{slow} (computationally intensive). Each step involves fitting increasingly complex mdoels.
        \item The regularisation path is \hl{unstable}. OOS predictions may change dramatically by including a new regressor. Also small changes to the data may lead to radically different regularisation paths, making model selection unreliable and hard to replicate. 
    \end{itemize}
\end{shaded}

Penalised regressions offer a way to get stable regularisation paths.

\subsection{Penalised Regressions}

The instability comes from the number of large coefficients in our models. Because $\hat{y} = \mathbf{x^\prime}\bm{\hat{\beta}}$, and we have large $\bm{\hat{\beta}}$ values, when we apply the model to new data $\hat{y}$ will jump around.

\begin{itemize}
    \item We want $\hat{\beta}$ to be \hl{less variable, give better predictions and contain exact zeros}.
\end{itemize}

The point of penalised regressions is to shrink the coefficients estimates (even to zero). This leads to bias, however we will have less volatile predictions.

\subsubsection{Bias-Variance trade-off}

The expected \textit{test MSE} for a given value $x_0$, can ALWAYS be decomposed into the sum of three fundamental quantities: the \hl{variance} of $\hat{f}(x_0)$, the \hl{squared bias} of $\hat{f}(x_0)$ and the \hl{variance of the error term $\epsilon$}. That is:

\begin{equation}
\label{expected test mse}
    \E[y_0 - \hat{f}(x_0)]^2 = var(\hat{f}(x_0)) + [\text{bias}(\hat{f}(x_0))]^2 + var(\epsilon)
\end{equation}

here the notation: $ \E[y_0 - \hat{f}(x_0)]^2$ defines the \textit{expected test MSE at $x_0$}, and refers to the average test MSE that would be obtained if we repeatedly estimated $f$ using a large number of training sets, and tested each at $x_0$.

\hl{The overall expected test MSE can be computed by averaging the expected test MSE over all possible values of $x_0$ in the test set.}

\begin{mdframed}
    \textit{Variance} refers to the amount by which our predicted response would change if we estimated it using a different training data set. Because the model is fit on the training data, by using different training data we will yield different predictions.\hl{Ideally we would not want the estimate for $f$ to vary much between training sets}.

    \textit{Bias} refers to the error that is introduced by approximating a real-life (complex) problem, by a much simpler model. For example, linear regressions assume a linear relationship between $Y$ and $\mathbf{x}$, however in real life the relationship may be highly non-linear. Therefore, by performing a linear regression will result in some bias in our predictions.
\end{mdframed}

Equation \eqref{expected test mse} tells us that to minimise the expected test error we select methods that simultaneously have \textbf{low variance} and \textbf{low bias}.

\begin{note}
    Expected test MSE can never lie below $var(\epsilon)$.
\end{note}

Generally, more flexible methods increase variance and decrease bias. \hl{The relative rate of change of these two quantities determines whether the test MSE increases or decreases as we increase flexibility}.
\begin{itemize}
    \item Initially, an increase in flexibility leads to a quicker decrease in bias than an increase in variance. \hl{Meaning expected test MSE declines}.
    \item At some point increasing flexibility has little impact on bias but starts to significantly increase variance and therefore \hl{increases expected test MSE}.
\end{itemize}

\subsection{Ridge and Lasso}

Recall, in the linear least squares model our $\beta$ estimates are such that they minimise the deviance:

\begin{equation*}
    \hat{\beta}_{LS} = \arg \underset{\beta}{\min} \left\{\sum_{i=1}^n(y_i - \mathbf{x_i^\prime}\bm{\beta})^2\right\}
\end{equation*}

Our least squares estimates can be unstable, prone to overfitting and cannot achieve exact zero estimates. Our \hl{penalised least squares} minimised deviance $+$ a penalty:

\begin{equation}
    \label{eq:pls}
    \hat{\beta}_{PLS} = \arg\underset{\beta}{\min}\left\{\sum_{i=1}^n(y_i - \mathbf{x_i^\prime}\bm{\beta})^2 + \lambda\sum_{k=1}^p c(\beta_k)\right\}
\end{equation}

where $\lambda >0$ is a penalty weight and $c(\beta_k)$ is cost function dependent on the estimated coefficients.

\subsubsection{Penalty Function}

The difference between ridge and lasso is the penalty function. To induce shrinkage, the smallest penalty must be when $\beta_k = 0$ and increasing in $|\beta_k|$.

\begin{itemize}
    \item \hl{Lasso}: $\hat{\beta}_{Lasso} = \arg\underset{\beta}{\min}\left\{\sum_{i=1}^n(y_i - \mathbf{x_i^\prime}\bm{\beta})^2 + \lambda\sum_{k=1}^p |\beta_k|\right\}$
    \item \hl{Ridge}: $\hat{\beta}_{Ridge} = \arg\underset{\beta}{\min}\left\{\sum_{i=1}^n(y_i - \mathbf{x_i^\prime}\bm{\beta})^2 + \lambda\sum_{k=1}^p (\beta_k)^2\right\}$
\end{itemize}

\begin{figure}[h]
    \centering
    \includegraphics[width=.75\textwidth]{pic/penalty weights.png}
    \caption{Lasso vs Ridge Penalty}
    \label{fig:penalty weights}
\end{figure}

\paragraph{Alternative Interpretation.} We can look at the penalised regressions as constrained optimization problems:

\begin{align*}
    \text{Lasso: } \underset{\beta}{\min}\left\{\sum_{i=1}^n(y_i - \mathbf{x_i^\prime}\bm{\beta})^2\right\} &\quad  \text{ s.t } \quad \sum_{k=1}^p |\beta_k| \leq S \\
    \text{Ridge: } \underset{\beta}{\min}\left\{\sum_{i=1}^n(y_i - \mathbf{x_i^\prime}\bm{\beta})^2\right\} &\quad  \text{ s.t } \quad \sum_{k=1}^p (\beta_k)^2 \leq S 
\end{align*}

For every $\lambda$ there is an $S$ such that, the estimated coefficients in the penalised regression and constrained optimisation are equal. $S$ can be thought of as a "coefficient budget"

\begin{mdframed}
    \textbf{\hl{Lasso is the best}}: it has the least bias on important regressors while retaining the stability of the ridge's convex penalty. It also induces automatic lasso selection as it sets irrelevant coefficients to \hl{exactly zero unlike ridge}.
\end{mdframed}

Because we are penalising the size of the regressors, we must scale them. We usually scale all regressors by their standard deviations and centre them on 0.

\begin{note}
    \hl{We need to keep all dummy variables in our penalised regressions}. If we were to have a reference variable, our regularisation path and eventual predictions would depend on which category was left out. When we keep the dummies we do not run into perfect multicollinearity.
\end{note}

\subsubsection{Creating the Regularisation Path}

By varying the penalty weight $\lambda$, we can see a path of candidate models. When $\lambda = 0$, we just have a conventional least squares model. As $\lambda \rightarrow \infty$ the penalised regression gives the null model. \hl{$\lambda$ is a hyperparameter}: external to the core prediction model, but controls the learning process.

We tend to pick the penalised regression model that minimises the OOS MSE. This model may be slightly overfit/too complex, so we also look at a conservative model, with fewer regressors, but higher MSE. This conservative model MSE is 1 standard error on the simple side.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{pic/lasso mse.png}
    \caption{OOS MSE along the regularisation path}
    \label{fig:lasso mse}
\end{figure}



\newpage
\section{Classification}

Most of the models we have made so far have been on continuous response variables. Many prediction problems are classification problems. We can fit the usual regression framework, but now our response variable is categorical.

\subsection{K-Nearest Neighbour Classification}

The $K$-NN algorithm predicts the class $\hat{y}$ from new data $\mathbf{x}$ by asking: \hl{what is the most common class for observations around $\mathbf{x}$}.

\begin{figure}[h]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{pic/image.png}
  \caption{3 Nearest Neighbour}
  \label{fig:3nn}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{pic/5nn.png}
  \caption{5 Nearest Neighbout}
  \label{fig:5nn}
\end{subfigure}
\caption{2 Examples of Nearest Neighbour Classification}
\label{fig:KNN}
\end{figure}


The new data point is assigned to the modal class of the K nearest data points. In Figure \ref{fig:KNN} (a) the new data point would be assigned to red, but in (b) the new data point would be classified as yellow.

\begin{algo}
    We have data $\{(\mathbf{x_i}, y_i)\}_{i=1}^n$ of labelled observations and input vector $\mathbf{x}_f$ of length $p$, where you would like to predict the class label.

    \begin{enumerate}
        \item Choose a $K$ value, for example $K=5$
        \item Find the $K$ (5) nearest neighbours in $\{(\mathbf{x_i}, y_i)\}_{i=1}^n$ using Euclidean distance:

        \begin{equation*}
            d(\mathbf{x_i, x_f}) = \sqrt{\sum_{j=1}^p (x_{ij}- x_{fj})^2}
        \end{equation*}
        \item Let $\left\{(\mathbf{x}_i^1, y_i^1), (\mathbf{x}_i^2, y_i^2), \ldots, \mathbf{x}_i^K, y_i^K)\right\}$, be the $K$ nearest neighbours of $\mathbf{x}_f$ in $\{(\mathbf{x_i}, y_i)\}_{i=1}^n$.
        \item The predicted class (classification) for $\mathbf{x_f}$ is
        \begin{equation*}
            \hat{y}_f = \operatorname{mode}\{y_i^1, y_i^2,\ldots, y_i^K\}
        \end{equation*}
    \end{enumerate}
\end{algo}

\subsubsection{Standardising Variables}

Because we are looking at Euclidean distance, the scale of each variable matters. If we were to multiply $x_1$ by 10, we would yield different predicted classifications. \hl{This is obviously not desirable}.

We standardise all inputs so distances are measured in standard deviations. This means that $K$-NN uses inputs:

\begin{equation*}
    \dfrac{x_j - \Bar{x}_j}{\sigma_{x_j}}, \quad \text{ for } j = 1, 2, \ldots, p
\end{equation*}

\begin{shaded}
    \textbf{Drawbacks of K-NN}:

    \begin{itemize}
        \item There is no uniformly good way of choosing $K$ in $K$-NN. We can try cross-validation, \hl{but it is unstable}: New data is likely to lead to new best $K$. 
        \item the classification is often very sensitive to the choice in $K$.
        \item $K$-NN gives very crude estimates of classification probabilities, either 0 or 1, \hl{which is useless for informing decision making}
        \item Without good probability estimates, \hl{we cannot assess mis-classification risk}.
        \item Computationally intensive as we need to count the nearest neighbours for each new $\mathbf{x}$.
    \end{itemize}
    
\end{shaded}

\subsection{Probabilistic Classification Models}

We will introduce the idea of costs in decision-making/classification. To simplify, we will look at the binary case, $y \in \{0,1\}$. There are two ways to be wrong in this case, you can have \hl{false positives or false negatives}. There may be different costs associated with making each type of error. We can generalise this with:
\begin{equation}
    \E[\operatorname{cost}(a)] = \sum_{k=1}^K p_k c(k,a)
\end{equation}

where $c(k,a)$ is a cost function. This equation highlights the fact that optimal decisions (minimising expected costs) require probabilities $p_k$.

\begin{example}
    A creditor may need a classification algorithm that provides good estimates of the probability that an applicant defaults if approved.
\end{example}

\subsubsection{Regression for binary responses}

A regression mosled the conditional mean $\E(y|\mathbf{x})$:
\begin{equation*}
    \E(y|\mathbf{x}) = f(\mathbf{x^\prime}\beta).
\end{equation*}

If $y \in \{0,1\}$ is a binary response variable, then given $p(\mathbf{x}) \equiv Pr(y = 1|\mathbf{x})$,
\begin{equation*}
    \E[y|\mathbf{x}] = 1 \times p(\mathbf{x}) + 0 \times (1-p(\mathbf{x}) = p(\mathbf{x})
\end{equation*}

We cannot just use a linear link function here, \hl{the linear regression model will yield probabilities outside the unit interval}. Additionally, in the general case, we cannot use linear regression methods with a qualitative response with more than two classes\footnote{pg 132 ISLR2}.

So, we need a link function $f(\cdot)$ that gives values between 0 and 1:
\begin{equation*}
    \E(y|\mathbf{x}) = p(\mathbf{x}) = f(\mathbf{x^\prime}\beta) = f(\beta_0 + \beta_1 x_1 + \ldots, \beta_p x_p) \in [0,1].
\end{equation*}

This gives rise to the logistic regression.

\subsubsection{Logistic Regression}

The logistic regression gives us a lin function where our predictions will be in the interval $[0,1]$.

\begin{equation}
    \label{eq:logistic link}
    f(\mathbf{x^\prime}\beta) = \dfrac{\exp((\mathbf{x^\prime}\beta))}{1 + \exp (\mathbf{x^\prime}\beta)}
\end{equation}

\begin{figure}[h]
    \centering
    \includegraphics[width=.25\linewidth]{pic/logistic.png}
    \caption{Logistic regression link function}
    \label{fig:enter-label}
\end{figure}

\paragraph{Interpreting the logistic regression}: \hl{the logistic regression is a linear model for log odds}. The odds of an event are the probability it happens over the probability that it doesn't. 

Here are the odds of $y=1$:
\begin{equation*}
    \dfrac{p(\mathbf{x})}{1-p(\mathbf{x})}
\end{equation*}

If $p(\mathbf{x}) = 1/4$, the odds are 1/3. We have a $Pr(y=1|\mathbf{x}) = 0.25$. We know then, that the probability of it not happening is 0.75. The ratio of the two is 1:3.

In our logistic regression, we are looking at the \hl{\textit{log} odds}:

\begin{equation}
    \log\left[\dfrac{p(\mathbf{x})}{1-p(\mathbf{x})}\right] = \mathbf{x^\prime}\beta = \beta_0 + \beta_1 x_1 + \ldots + \beta_p x_p
\end{equation}

$\beta_k$ measures the percentage increase in the \hl{odds} for the event $y = 1$ associated with a unit increase in $x_k$.

\begin{procedure}
    Fitting a logistic regression model:

    Interpreting $n$ i.i.d. data by logistic regression lives the likelihood function:

    \begin{equation}
        \label{eq:likelihood}
        L(\beta) = \prod_{i=1}^n Pr(y_i|\mathbf{x}_i) = \prod_{i=1}^n \left[\dfrac{\exp(\mathbf{x_i^\prime}\beta)}{1 + \exp (\mathbf{x_i^\prime}\beta)}\right]^{y_i}\left[\dfrac{1}{1 + \exp (\mathbf{x_i^\prime}\beta)}\right]^{1-y_i}
    \end{equation}

    For a given observation $i$ where $y = 1$, the second term simplifies to 1 (the whole term is raised to the power of 0), and we only look at the probability of success.

    We tend to look at the \hl{log-likelihood} function as it is easier to take the derivative of. Which can be written as:

    \begin{equation}
        \label{eq:log-likelihood function}
         \mathcal{L}(\beta) = \sum_{i=1}^n \left[y_i \log (\exp(\mathbf{x_i^\prime}\beta)) - \log (1 + \exp(\mathbf{x^\prime}\beta))\right]
    \end{equation}

    As we are using a maximum likelihood estimator here, we take the derivative w.r.t $\beta$, so the maximum likelihood estimator of $\beta$ solves:
\begin{equation}
    \label{eq: max likelihood}
    \dfrac{\partial \mathcal{L}(\beta)}{\partial \beta} = \sum_{i=1}^n \left[y_i - \dfrac{\exp(\mathbf{x_i^\prime}\hat{\beta})}{1 + \exp (\mathbf{x_i^\prime}\hat{\beta})}\right] = 0 
\end{equation}
    
\end{procedure}

The \hl{saturated model} perfectly replicates the data i.e. $p_{s,i} = y_i$:

\begin{equation*}
    L_s = \prod_{i=1}^n p_{s,i}^{y_i} (1 - p_{s,i})^{1-y_i} = \prod_{i=1}^n y_{i}^{y_i} (1 - y_{i})^{1-y_i} =1
\end{equation*}

We get this result because $0^0 \equiv 1$. We therefore get the \hl{logistic deviance}:

\begin{equation*}
    D(\beta) = -2(\log(L(\beta) - \underbrace{\log L_s}_{=0}) \propto \sum_{i=1}^n \left[\log(1 + e^{\mathbf{x_i^\prime}\beta}) - y_i \mathbf{x_i^\prime}\beta\right]
\end{equation*}

\subsubsection{Penalised Logisitic Regression}

In the same way that linear regression can become overfitted, our logistic regression can too. We can apply the same ideas to logistic regression, namely \textit{penalised} logistic regressions.

In the logistic case, we are minimising the penalised deviance defined as:

\begin{equation}
    \label{eq:log dev pen}
    D_{pen}(\beta,\lambda) \propto \sum_{i=1}^n\left[\log(1 + e^{\mathbf{x_i^\prime}\beta}) - y_i \mathbf{x_i^\prime}\beta\right] + \lambda \sum_{k=1}^p c(\beta_k)
\end{equation}

where, again, $c(\beta_k)$ is the cost function dependent on our coefficients, and $\lambda$ is a hyperparameter penalty weight. 

\begin{note}
    Similar to penalised linear regressions, minimising penalised deviance shrinks fitted coefficients towards zero.
\end{note}

\subsection{Probabilities and Classification}

A \hl{classification rule} is the probability cutoff $p^*$ such that
\begin{equation*}
    \hat{y} = \begin{cases}
        1, &\text{ if } \hat{p} > p^* \\
        0, &\text{ if } \hat{p} \geq p^*
    \end{cases}
\end{equation*}

There are two types of classification errors:
\begin{enumerate}
    \item \hl{False Positive:} classify a true negative ($y=0$) as a positive ($\hat{y} = 1$)
    \item \hl{False Negative:} classify a true positive $(y = 1$) as a negative $(\hat{y} = 0)$
\end{enumerate}

The classification rule \hl{that we set} should make as few errors as possible, but how do we identify a good rule?

\begin{enumerate}
    \item \hl{False Positive Rate:} Proportion of observations that were classified as positive ($\hat{y} = 1$) that are true negatives ($y = 0$)
    \item \hl{False Negative Rate:} Proportion of observations that were classified as negative ($\hat{y} = 0$) that were true positives ($y=0$)
    \item \hl{Sensitivity:} Proportion of true positives classified correctly
    \item  \hl{Specificity:} Proportion of true negatives classified correctly
\end{enumerate}

As you can imagine there is a relationship between FNR/FPR and sens/spe. As the specificity is the proportion of true negatives, 1 - specificity is the false negative rate. Similarly, with sensitivity, 1 - sensitivity is the false positive rate.

to visualise these metrics we can create some graphs.


\begin{figure}[h]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{pic/error rates.png}
  \caption{FNR/FPR with changing $p^*$}
  \label{fig:3nn}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{pic/roc.png}
  \caption{ROC Curve}
  \label{fig:5nn}
\end{subfigure}
\caption{Classification Metrics}
\label{fig:classification metrics}
\end{figure}

It is difficult to tell what the optimum classification rule is from plot (a) of Figure \ref{fig:classification metrics}, but when we look to (b) we are aiming to be in the top-left. The top-left of the ROC plot is the ideal model with a 100\% sensitivity and a 100\% specificity. We can think of the horizontal axis of the ROC plot as the FNR. Meaning we want a model with an FNR of 0. 

\newpage

\section{Tree-Based Methods}

Tree-based methods are simple and useful for interpretation but are usually not competitive with the best supervised learning approaches. We start to introduce techniques that make drastic improvements to the simple decision tree, like bagging and random forests.


\subsection{Decision Trees}

A logical system to map regressors to outcomes. They are \hl{hierarchical} meaning they use a series of ordered steps to come to a conclusion. We will be using data on baseball player salaries.

To illustrate, here is a decision tree:

\begin{figure}[h]
    \centering
    \includegraphics[width=6cm]{pic/decision tree.png}
    \caption{Decision Tree}
    \label{fig:decision tree}
\end{figure}

In this example, we are predicting player salaries and using the regressor \lstinline{hits} and \lstinline{years}. This decision tree can be visualised like so:

\begin{figure}[h]
    \centering
    \includegraphics[width=6cm]{pic/decision tree parts.png}
    \caption{Decision Tree as Data}
    \label{fig:decision tree parts}
\end{figure}

At our \hl{primary node} we check whether the player has been in the league for less than 4.5 years. If so, we continue on the left. If the player has been in the league for over 4.5 years, then we continue to the next node on the right (hits $< 117.5$). The $R_1$ section in Figure \ref{fig:decision tree parts} corresponds to the left-most \hl{leaf} in Figure \ref{fig:decision tree}. $R_2$ corresponds to the middle leaf and $R_3$ corresponds to the right-most leaf. 

\begin{note}
    From both these Figures (\ref{fig:decision tree} \ref{fig:decision tree parts}) we can infer that, the average (log) salary for a player with less than 4.5 years of experience is 5.11; a player with more than 4.5 years of experience but less than 117.5 hits has an average (log) salary of 6.00; and a player with more than 4.5 years experience and more than 117.5 hits has an average (log) salary of 6.74.
\end{note}

\begin{shaded}
    \textbf{Terminology:}

    The tree splits input space $\mathbf{x} = \text{years, hits})$ in final nodes called \textbf{leafs}:

    \begin{itemize}
        \item $R_1 = \{\mathbf{x}|\text{years} < 4.5\}$
        \item $R_2 = \{\mathbf{x}|\text{years} \geq 4.5, \text{hits}<117.5\}$
        \item $R_3 = \{\mathbf{x}|\text{years} \geq 4.5, \text{hits}\geq 117.5\}$
    \end{itemize}

    Points on the tree where input space is split are called \textbf{internal nodes}. Segments connecting the nodes are \textbf{branches}.
\end{shaded}

So far we have an extremely simple tree, so we must grow it.

\subsubsection{Growing the Tree}

\begin{definition}
    Split $\mathbf{x}$-space in $J$ distinct, non-overlapping regions, $R_1, R_2, \ldots, R_J$. Suppose $\mathbf{x}_i \in R_j$; then the prediction for $y_i$ is $\hat{y} = \Bar{y} \equiv \frac{1}{n_j}\sum_{k:\mathbf{x}_k\in R_j} y_k$. Where $n_j$ is the number of observations that belong to $R_j$.

    \begin{note}
        This just means that for a given set of nodes (classifiers), our prediction will be the average of all the observations that fit said classifiers.
    \end{note}

    $R_1, R_2, \ldots, R_J$ are chosen to minimise the \textbf{loss function} (deviance).

    \begin{itemize}
        \item When $y$ is real: loss is the sum of squared errors (=regression deviance)

        \begin{equation*}
            \sum_{i=1}^n(y_i - \hat{y}_i)^2 = \sum_{j=1}^J \sum_{k:\mathbf{x}_k\in R_j}(y_k - \Bar{y})
        \end{equation*}

        \item When $y$ is binary: loss is the Gini impurity (or logit deviance)

        \begin{equation*}
            \sum_{i=1}^n \hat{y}_i(1-\hat{y}_i)^2 = \sum_{j=1}^J \sum_{k:\mathbf{x}_k\in R_j} \Bar{y}_j(1-\Bar{y}_j)^2
        \end{equation*}
    \end{itemize}
\end{definition}

Many ways to describe this algorithm:

The algorithm is \hl{greedy} as it makes the \textbf{locally} best choice at every step (similar to the forward step-wise regression). \hl{top-down} as it starts at the top of the tree. \hl{Recursive} as it solves the same problem in each step.

\subsubsection{Recursive Binary Splitting}

The node, $m$, is not split (leaf) it contributes cost:

\begin{equation*}
    \text{non-split node-}m \text{ cost} = \sum_{k:\mathbf{x}_k\in R_m}(y_k - \hat{y}R_m)^2
\end{equation*}

What the algorithm is doing is finding $x_j \in \{x_1, \ldots, x_p\}$ and cutoff $s$ to split $\mathbf{x}$-space into node-$m$ children

\begin{equation*}
    R_{left} = \{\mathbf{x}|x_j<s\}; \qquad R_{right} = \{\mathbf{x}|x_j \geq s\}
\end{equation*}

to minimise our cost function:

\begin{equation*}
    \text{split-node-}m \text{ cost} = \underbrace{\sum_{k:\mathbf{x}_k\in R_{left}}(y_k - \hat{y}R_{left})^2}_{\text{left cost}} + \underbrace{\sum_{k:\mathbf{x}_k\in R_{right}}(y_k - \hat{y}R_{right})^2}_{\text{right cost}}
\end{equation*}

So we are determining which parent node $m$ gives the maximum cost reduction by splitting, \hl{i.e. which regressor and cutoff reduces the cost function the most}. We keep doing this until a \textbf{convergence criterion} is met. This can be that each region (leaf) contains at least 3 observations. Or each split has to contribute a minimum decrease in deviance e.g. 0.001.


    
\begin{algo}

Tree creation:

\begin{center}

\begin{tikzpicture}[SIR/.style={rectangle, draw=red!60, fill=red!5, very thick, minimum size=5mm},]
%Nodes
\node[SIR] (1)     {Find optimal split - $x_j$ and cutoff $s$ to minimise loss};
\node[SIR] (2)[below=of 1] {Split this parent node into the left and right children};
\node[SIR] (3) [below=of 2] {each of these children are now parent nodes};

%Lines
\draw[->, very thick] (1.south)  to node[right] {} (2.north);
\draw[->, very thick] (2.south)  to node[right] {} (3.north);
\draw[->, very thick] (3.east) .. controls  +(right:7mm) and +(right:7mm)   .. (1.east);

\end{tikzpicture}
\end{center}
    
\end{algo}


This continues recursively until you reach a leaf node of some prespecified minimum size.

\subsubsection{Overfitting Our Tree}

\hl{When our tree is deep, it tends to be overfit}. The nonparametric nature of tree regressions means that it is very easy to overfit the data with an overgrown tree, leading to poor OOS predictions. An overgrown tree will be able to fit nonlinear mean and interaction effects without having to specify them (unlike linear regressions).

We have to prune the tree to constrain the flexibility and get better OOS predictions. We do this by:
\begin{itemize}
    \item construct a regularisation path of trees (from deep to shallow)
    \item Cross validate models along the path
\end{itemize}

\subsection{Cost Complexity Pruning}

A smaller tree with few splits (fewer regions) might lead to lower variance and better interpretation at the cost of a little bias. If we were to set the constraints very high (high minimum decrease in deviance) we may accidentally grow the tree too short and we miss a large reduction in RSS because of a small decrease before it. 

To avoid this we can grow a very complex and deep tree and then \textit{prune} it back in order to obtain a \hl{subtree}.

\begin{definition}
    \hl{Cost Complexity Pruning selects a sequence of subtrees} indexed by $\alpha>0$. Let $T_0$ be the complex tree; for each $\alpha>0$ find subtree $T_\alpha\subset T_0$ such that

    \begin{equation}
        \sum_{j=1}^{|T_\alpha}\sum_{k:\mathbf{x}_k\in R_k}(y_k - \Bar{y}_j)^2 + \alpha|T_\alpha|
    \end{equation}

    is minimised.

    \begin{note}
        $|T_\alpha|$ s the number of leaves on $T_\alpha$
    \end{note}

    Our \textbf{hyper-parameter} $\alpha$ penalises complexity (number of leaves); by varying $\alpha$ we obtain a regularisation path from complex to simple.

    \begin{note}
        $\alpha=0$ gives us the full tree. $\alpha\rightarrow\infty$ gives us a tingle leaf, $|T| = 1$.
    \end{note}

    We evaluate the OOS deviance for every tree on the path by cross-validation, then pick the subtree with the minimum OOS deviance.
\end{definition}



\subsubsection{Pros and Cons of Trees}

\begin{shaded}
    Pros:
    \begin{itemize}
        \item[+] Easy to explain (probably easier than regressions)
        \item[+] Can be argued that decision trees more closely mirror human decision making than regressions and classification methods
        \item[+] Easy to graph and interpret
        \item[+] Can easily handle qualitative regressors without the need for dummy variables.
    \end{itemize}

    Cons:
    \begin{itemize}
        \item[-] Do not tend to have the same predictive accuracy that regression models have 
        \item[-] Can be very unstable. Small changes in the data lead to drastic changes in the tree.
    \end{itemize}
\end{shaded}


\subsection{Ensemble Methods}


Trees can easily lead to overfitting. You \textit{can} do model selection by cross-validation, but the results are unstable and thus unreliable and impractical. Because there are no coefficients, we can't penalise the coefficients like we did before (we have now penalised the number of coefficients). 

Instead, we grow a \hl{forest} (= many trees) and regularise by \hl{bagging to obtain the random forest predictor}.



An ensemble method combines many simple models (aka "weak learners") to obtain a single powerful model. We look at both \hl{bagging and random forest} as ensemble methods that give better predictive power and stability.

\begin{note}
    We often see that when we use the ensemble approach there is usually a loss of interpretation in the final model.
\end{note}

\subsubsection{Bagging}

An overgrown tree is typically overfitted i.e. low bias and high variance. Bagging offers a way of reducing the variance of a tree based on the idea that \textbf{averaging reduces variance}.

\begin{definition}
    Bagging:

    Suppose we have $B$ training datasets so we could grow $B$ trees. Given inputs $\mathbf{x}$, we can make $B$ predictions $\hat{y}^b(\mathbf{x})$; averaging yields

    \begin{equation*}
        \hat{y}_{avg}(\mathbf{x}) = \dfrac{1}{B}\sum_{b=1}^B \hat{y}^b(\mathbf{x})
    \end{equation*}

    which is the \textbf{bagging predictor}. 
\end{definition}

Bagging is an average of i.i.d trees and the bagging predictor has lower variance than each individual tree. We do not have $B$ datasets, we only have 1 so we \hl{resample with replacement} to create $B$ datasets.

\begin{note}
    resampling with replacement means that an individual observation can be sampled multiple times in one dataset.
\end{note}
\subsubsection{Random Forests}

Random forest is a slightly modified bagging procedure. The splits in the fitted trees are chosen optimally across a random sample of inputs (meaning at on a specific tree, each node only has the choice of a few of the original regressors to make the split).

It is this random selection of predictors at each node that reduces the correlation between the trees grown at the different bootstrapped training samples. This decorrelation may induce some bias in the prediction, it also reduces the prediction variance and often improves the prediction accuracy.

\subsubsection{Out-Of-Bag Error}

We can estimate the the test MSE by cross validation but for bagging-based predictors there is an easier way: \textbf{out-of-bag error} (OOB error).

It can be shown that each bagged tree utilises only around two-thirds of the observations. The remaining one-third of the observations not used to fit a given bagged tree are referred to as the out-of-bag observations. We can predict the response for the $i$th observation using each of the trees in which that observation was OOB. This will yield around $B/3$ predictions which we can then average over (for regressions) or take majority vote (classification models).

\section{Unsupervised Learning}

So far, our main objective has been dimension reduction on big data  i.e. learn low dimensional summary stats necessary for good decisions from high dimensional input $\mathbf{x}$. And \hl{so far, this has all be supervised}, i.e. we have a target variable $y$ (regress target $y$ on high dimensional $\mathbf{x}$ ti get one dimensional $\hat{y} = \mathbf{x}^\prime\hat{\beta}$).

\hl{In unsupervised machine learning there is no target or outcome variable.} We still have a high dimensional input $\mathbf{x}$, and we still want dimension reduction. It is common to find partially labelled data and in this case we often use unsupervised learning to split the data into groups; then use the labelled data to predict.

\subsubsection{Mixture models}

Cluster analysis is used to collect similar observations into groups. Suppose data $(\mathbf{x}_i)_{i=1}^n$ where $\mathbf{x} = (x_1, x_2, \ldots, x_p)$ is a vector with $p$ characteristics. Clustering represents the data as output from a mixture distribution.

\paragraph{Mixture Distribution}: $\mathbf{x}_i$ is drawn from one of $K$ \textbf{mixture components} $p_k(\mathbf{x}), k = 1, 2, \ldots, K$. These mixture components are multivariate distributions of $\mathbf{x}$. 

The \textit{Guassian mixture model} where $p$ characterises $\mathbf{x}$ are i.i.d with variance $\sigma_k^2$ is a common clustering model;

\begin{equation}
    p_k(\mathbf{x}_i) = \dfrac{1}{(2\pi\sigma_k^2)^{p/2}}\exp\left[-\dfrac{\sum_{j=1}^p (x_{ij}-\mu_{kj})^2}{2\sigma_k^2}\right]
\end{equation}

With our unlabelled data we do not know which of $K$ components $\mathbf{x}_i$ is from. We only see the characteristics of the observations:

\begin{equation*}
    p(\mathbf{x}_i) = \pi_1 p_1(\mathbf{x}_i) + \pi_2 p_2 (\mathbf{x}_i) + \ldots + \pi_k p_k (\mathbf{x}_i)
\end{equation*}

where $\pi_k$ is the probability that $\mathbf{x}_i$ comes from the component $k$.

\subsection{Factorisation by K-Means}

We look at K-means as a way to model mixture models. This means the data comes from differing distributions and because of this it is important to group the data to match this.

K-means clustering is a way of grouping observations and it comes from a simple and intuitive maths problem.

\begin{definition}
    Let $C_1, C_2, \ldots, C_K$ denote sets containing the indices of the observations in each cluster. These sets satisfy

    \begin{enumerate}
        \item $C_1 \cup C_2 \cup \ldots \cup C_K = \{1, \ldots, n\}$. i.e. all observations are accounted for.

        \item $C_k \cap C_{k^\prime} = \varnothing \quad \forall k \neq k^\prime$. i.e. clusters do not overlap. No observation belongs to multiple clusters.
    \end{enumerate}
\end{definition}

\begin{note}
    With K-mean clustering, we specify a desired number of clusters $K$. 
\end{note}

\textbf{Good} clustering is one for which the \textit{within-cluster variation} is as small as possible. The within-cluster variation for cluster $C_k$ is a measure $W(C_k)$ of the amount by which the observations within the cluster differ from each other. Hence we want to solve this problem:

\begin{equation*}
    \underset{C_1, \ldots, C_K}{\min}\left\{\sum_{k=1}^K W(C_k)\right\}
\end{equation*}

We ant to group the observations into $K$ clusters such that the total within-cluster variation, summed over all $K$ clusters, is as small as possible. %This within-cluster variation can be calculated in many ways, but we use the \textit{squared Euclidean distance}:

\begin{algo}
    K-means:

    \begin{center}
\begin{tikzpicture}[SIR/.style={rectangle, draw=red!60, fill=red!5, very thick, minimum size=5mm},]
%Nodes
\node[SIR] (1)     {Randomly assign each observation to a cluster ($1,\ldots, K)$};
\node[SIR] (2)[below=of 1] {Compute the cluster \textit{centroid}};
\node[SIR] (3) [below=of 2] {Assign each observation to the cluster whose centroid is closest};

%Lines
\draw[->, very thick] (1.south)  to node[right] {} (2.north);
\draw[->, very thick] (2.south)  to node[right] {} (3.north);
\draw[->, very thick] (3.east) .. controls  +(right:7mm) and +(right:7mm)   .. (2.east);

\end{tikzpicture}
\end{center}
\end{algo}

The cluster centroid is defined as:

\begin{equation*}
    \hat{\mu}_k = \Bar{x}_k = \dfrac{1}{n_k}\sum_{i:k_i = k}x_i
\end{equation*}
where $i: k_i = k$ are the $n_k$ observations with $k_i = k$.

In our example, $W(C_k)$ is the deviance:

\begin{equation*}
    W(C_k) = D_k = \sum_{j=1}^p (X_{ij} - \mu_{kj)^2}
\end{equation*}

Thus, our total deviance of $\{\mathbf{x}_i\}_{i=1}^n$ is 

\begin{equation*}
    D = \sum_{i=1}^n \sum_{k=1}^K 1_{k_i = k} \sum_{j=1}^p (x_{ij} - \mu_{kj})^2
\end{equation*}

\hl{But we do not know the cluster assignment $\{k_i\}_{i=1}^n$}. To minimise this deviance we have to estimate both the centroids and the assignment.

Objective: fit $\{\mu_k\}_{k=1}^K$ and $\{k_i\}_{i=1}^n$ to minimise the deviance

\begin{equation}
    \underset{\{\mu_k\}_{k=1}^K,\{k_i\}_{i=1}^n}{\min}\left(\sum_{i=1}^n \sum_{k=1}^K 1_{k_i = k} \underbrace{\left[\sum_{j=1}^p (x_{ij} - \mu_{kj})^2\right]}_{\text{within-cluster SS}}\right)
\end{equation}

where $k_i \in \{1,2,\ldots, K\}$ is the cluster assignment of observation $i$.

We can visualise this with the following figure

\begin{figure}[h]
    \centering
    \includegraphics[width=13cm]{pic/k-means algo.png}
    \caption{Steps of the K-means algorithm}
    \label{fig:k-means algo}
\end{figure}

We repeat the algorithm until the cluster assignment no longer changes (convergence).

\subsubsection{Cons of K-means}

\begin{shaded}
    Drawbacks of the K-means algorithm:

    \begin{itemize}
        \item[-] How do we choose $K$. It is highly subjective.
        \item[-] Algorithm may converge on a local minima and not the global minima.
        \item[-] Local minima + initial randomisation means that multiple runs of the K-means algorithm on the same data may yield different results. 
    \end{itemize}

    To get around this we tend to do many runs with different initial random clusters and pick the run with the smallest sum-squared error.
\end{shaded}

It is common practice to report both the within- and between-cluster SS:

\begin{equation}
    \text{TSS} = \underbrace{\sum_{k=1}^K \sum_{i=1}^n 1 _{k_i = k}\left[\sum_{j=1}^p (x_{ij} - \mu_{kj})^2\right]}_{\text{\textbf{within}-cluster SS}} + \underbrace{\sum_{k=1}^K \sum_{i=1}^n 1 _{k_i = k}\left[\sum_{j=1}^p (\mu_{kj} - \mu_{j})^2\right]}_{\text{\textbf{between}-cluster SS}}
\end{equation}





\subsection{Factorisation by Principal Components}

\subsubsection{Factor Models}

Suppose we have data $\{\mathbf{x}_i\}_{i=1}^n$ and $\mathbf{x}$ is a $p$-dimensional vector where $p$ is large. We may want to reduce $\mathbf{x}$ to a few important factors.

\begin{definition}
    Factor Model:

    $x_{ij}$ is a linear model of $K$ scalar factors $v_{i1}, v_{i2}, \ldots, v_{iK}$:

    \begin{equation*}
        \E(x_{ij}) = \varphi_{j1}v_{i1} + \varphi_{j2}v_{i2} + \ldots + \varphi_{jK}v_{iK}; \qquad i = 1,2,\ldots, n
    \end{equation*}
    where $\varphi_{jk}$ is a coefficient and $v_{ik}$ is amount of factor-$k$ in observation $i$.

    \begin{note}
        Factor $v_{ik}$ is unobserved (=latent)
    \end{note}

    $(\mathbf{v_1, v_2, \ldots, v_K})$ is a set of low-dim regressors that capture the "essence" of $\mathbf{x}$. The $\varphi_{jk}$'s are the \textbf{loadings} or \textbf{rotations} that sets how much $v_{ik}$ goes into $x_{ij}$.
\end{definition}


We can rewrite this in vector notation:

\begin{equation*}
    \E(\mathbf{x}_i) = \varphi_1 v_{i1} + \varphi_2 v_{i2} + \ldots + \varphi_K v_{iK}
\end{equation*}

\begin{note}
    $\mathbf{x}_i =  (x_{i1}, x_{i2}, \ldots, x_{ip})$ is $p$-vector. And each observation $i$ has $K$ factors, $v_{i1}, v_{i2}, \ldots, v_{iK}$. $v_{ik}$ is univariate whereas $\mathbf{v}_i = (v_{i1}, v_{i2}, \ldots, v_{iK})$ is a $K$-vector. The rotation vector $\varphi_k = (\varphi_{1k}, \varphi_{2k}, \ldots, \varphi_{pk})$ is a $p$-vector which translates the simple $K$-vector $\mathbf{v}_i$ to the complex $p$-vector $\mathbf{x}_i$.
\end{note}

\paragraph{Mixture models are Factor models!} $K$-means was motivated by a mixture model and is a special case of a factor model. However, mixture models only load on a single factor. \hl{Factor models allow for mixed membership: $x_{ij}$ don't have to be from one component, but can be a mix of shared latent factors.}


\subsection{PCA}
\label{sec:PCA}

PCA aims to find a low-dimensional representation of data that captures as much variance (information) as possible.

When faced with a large set of correlated variables, principle components allow us to summarise this set with a smaller number of representative variables that collectively explain most of the variability in the original set. \footnote{More info on PCA in Section \ref{appendix: dimension reduction}.}

PCA refers to the process of computing these components and using them to understand the data. This is an unsupervised approach as we only have the set of features $\mathbf{x_1, x_2, \ldots, x_p}$ and no associated response $\mathbf{y}$.

\subsubsection{2 variable example}

First, we will go over the example of $p=2$ variables.

\begin{figure}[h]
    \centering
    \includegraphics{pic/pca 2p.png}
    \caption{Left: our variables as axes; Right: Principle Components as axes}
    \label{fig:pca2}
\end{figure}

In the left plot, our first principle component ($PC_1$) is shown as the diagonal line. Additionally, we can see that this line has the most variance, i.e. if it was any other line, our points in the right plot would be more bunched together. Again, once we have decided our $PC_1$, $PC_2$ will be the line that has the next most uncorrelated variance, and this by construction is the orthogonal line.

\begin{note}
    by construction in the 2 variable case, we will only have 2 principle components and they will be orthogonal. This means that $PC_2$ will be perpendicular to the diagonal line in the left plot. This is why we can plot $PC_1$ against $PC_2$ in the right plot.
\end{note}

\begin{procedure}
    The first Principle Component, $v_i^1$, is a weighted average across $x_{i1}$ and $x_{i2}$ with the maximum variance across the $n$ observations. That is:

    \begin{equation}
        \underset{\varphi_1, \varphi_2}{\max}\underbrace{\sum_{i=1}^n (v_i^1)^2}_{\text{variance of PC1}} = \underset{\varphi_1, \varphi_2}{\max} \sum_{i=1}^n (\varphi_1 x_{i1} + \varphi_2 x_{i2})^2
    \end{equation}

    We constrain the maximisation problem with the following:
    \begin{equation*}
        \varphi_1^2 + \varphi_2^2 = 1
    \end{equation*}

    When choosing $\varphi_1, \varphi_2$ we are setting the direction in which we want to add variables, i.e. the rotation of the coordinate system.

    \begin{note}
        If we look back at Figure \ref{fig:pca2}, we can see that the transformation from the left plot to the right is the rotation of the coordinate system.
    \end{note}

    We then take out PC1, and work with the residualised data:

    \begin{equation*}
        \{\Tilde{x}_{i1}, \Tilde{x}_{i2}\}_{i=1}^n = \{x_{i1} - \varphi_1 v_{i1},x_{i2} - \varphi_2 v_{i1}\}_{i=1}^n
    \end{equation*}
    
\end{procedure}

\begin{example}
    When the maximum spread line is the 45 degree line, one can show that $\varphi_1 = \varphi_2 = \sqrt{1/2}$. That is:

    \begin{equation*}
        v_{i1} = \sqrt{\dfrac{1}{2}}(x_{i1} + x_{i2})
    \end{equation*}

    When we take out PC1, we are left with the residualised data:

    \begin{align*}
        \{\Tilde{x}_{i1}, \Tilde{x}_{i2}\}_{i=1}^n &= \{x_{i1} - \varphi_1 v_{i1},x_{i2} - \varphi_2 v_{i1}\}_{i=1}^n \\
        &= \{x_{i1} - \sqrt{\frac{1}{2}} v_{i1},x_{i2} - \sqrt{\frac{1}{2}} v_{i1}\}_{i=1}^n \\
        &= \left\{x_{i1} - \sqrt{\frac{1}{2}} \left(\sqrt{\frac{1}{2}}(x_{i1} + x_{i2})\right),x_{i2} - \sqrt{\frac{1}{2}} \left(\sqrt{\frac{1}{2}}(x_{i1}+x_{i2})\right)\right\}_{i=1}^n \\
         &= \left\{x_{i1} - \frac{1}{2}(x_{i1} + x_{i2}),x_{i2} - \frac{1}{2}(x_{i1}+x_{i2})\right\}_{i=1}^n \\
         &= \left\{\frac{1}{2}(x_{i1} + x_{i2}),\frac{1}{2}(x_{i2}-x_{i1})\right\}_{i=1}^n
    \end{align*}

    Then, our second principle component $v_{i2}$ is a weighted average of $\Tilde{x}_{i1}, \Tilde{x}_{i2}$, with the maximum residual variance across the $n$ observations. That is:

    \begin{equation}
        \underset{\varphi_1, \varphi_2}{\max}\underbrace{\sum_{i=1}^n (v_i^2)^2}_{\text{variance of PC2}} = \underset{\varphi_1, \varphi_2}{\max} \sum_{i=1}^n (\varphi_1 \Tilde{x}_{i1} + \varphi_2 \Tilde{x}_{i2})^2
    \end{equation}

    We find that when $\varphi_1 = \varphi_2 = \sqrt{\frac{1}{2}}$:
    \begin{align*}
    v_{i1} &= \sqrt{\frac{1}{2}}(x_{i1} + x_{i2}) \\
    v_{i2} &= \sqrt{\frac{1}{2}}(x_{i1} - x_{i2})
    \end{align*}
\end{example}

\subsubsection{Variance Accounting}

We will continue with the example in which $\varphi_1 = \varphi_2 = \sqrt{\frac{1}{2}}$. That is, the maximum variance line is the 45 degree line. We can find the variance of $v_{i1}$ and $v_{i2}$.

\begin{align*}
    \sum_{i=1}^n (v_{i1})^2 &= \frac{1}{2}\sum_{i=1}^n(x_{i1} + x_{i2})^2 = \frac{1}{2}\sum_{i=1}^n x_{i1} + \frac{1}{2}\sum_{i=1}^n x_{i2} + \sum_{i=1}^n x_{i1} x_{i2} \\
    \sum_{i=1}^n (v_{i2})^2 &= \frac{1}{2}\sum_{i=1}^n(x_{i1} + x_{i2})^2 = \frac{1}{2}\sum_{i=1}^n x_{i1} + \frac{1}{2}\sum_{i=1}^n x_{i2} - \sum_{i=1}^n x_{i1} x_{i2}
\end{align*}

\begin{shaded}
    Our observations:
    \begin{itemize}
        \item If $x_{i1}$ and $x_{i2}$ are correlated, the first principal component always has higher variance than the second.

        \item Together, the principal components explain all of the variance in $\{x_{i1}, x_{i2}\}_{i=1}^n$.
    \end{itemize}

    \hl{these observations generalise to factor models with $p$ variables.}
    
\end{shaded}

\subsubsection{General Model}

\begin{algo}

\begin{enumerate}
\hspace{1in}
    \item Set $\mathbf{\Tilde{X}^1 = X}$, an $n \times p$ matrix. Then for $k = 1, 2, \ldots, \min(n,p)$. 
    \item Find:
    \begin{equation}
        \varphi_k = \arg\underset{\varphi}{\max}\left\{\mathbb{V}ar\left(\mathbf{\Tilde{X}^1}\phi\right) = \mathbb{V}ar(v_{k1}, v_{k2}, \ldots, v_{kn})\right\}
    \end{equation}
    where $v_{ki} = \phi^\prime \mathbf{x}_i = \sum_{j=1}^px_{ij}\phi_{jk}$ and $\sum_{j=1}^p \phi_{jk}^2 = 1$
    \item Update rows of $\mathbf{\Tilde{X}^{k+1}}$ via $\mathbf{\Tilde{x}_i^k} - v_{ik} \times \varphi_k$
\end{enumerate}

\end{algo}

The PCA algo repeatedly fits both rotations and factors to minimise deviance across dimensions $j=1,2, \ldots, p$ and obs $i=1,2, \ldots, n$ for factor model

$$
\tilde{x}_{i j}^{k} \sim \mathcal{N}\left(v_{i k} \varphi_{j k}, \sigma_{k}^{2}\right)
$$
where $\tilde{x}_{i j}^{k}=\tilde{x}_{i j}^{k-1}-v_{i, k-1} \varphi_{k-1, j}$.

\begin{note}
    We haven't exactly described the algorithm, but we have described the intuition. In practice, the algo is minimising deviance.

$$
\sum_{j=1}^{p} \sum_{i=1}^{n}\left(x_{i j}-\sum_{k=1}^{K} \phi_{j k} v_{i k}\right)^{2}
$$
\end{note}

\subsubsection{Relationship between PCA and Factor Models}

The factor model interprets the data as

$$
\mathbb{E}\left(\mathbf{x}_{i}\right)=\varphi_{1} v_{i 1}+\varphi_{2} v_{i 2}+\ldots+\varphi_{K} v_{i K}=\Phi \mathbf{v}_{i} ; \quad i=1,2, \ldots, n
$$
Applying PCA we look for principal components such that

$$
v_{i k}=\varphi_{1 k} x_{i 1}+\varphi_{2 k} x_{i 2}+\ldots+\varphi_{p k} x_{i p}=\mathbf{x}_{i}^{\prime} \varphi_{k} ; \quad i=1,2, \ldots, n ; k=1,2, \ldots, K
$$

Stacking the $K$ PCs $\mathrm{PC}_{i}=\left(P C_{i 1}, P C_{i 2}, \ldots, P C_{i K}\right)$ implies

$$
\mathbf{P C}_{i}=\Phi^{\prime} \mathbf{x}_{i} ; \quad i=1,2, \ldots, n
$$

where $\Phi=\left(\varphi_{1}, \varphi_{2}, \ldots, \varphi_{K}\right)$ is a $p \times K$ matrix of PCA rotations.

By construction, the PCA rotation matrix is orthogonal $\Phi^{\prime} \Phi=\mathbf{I}$; hence,

$$
\Phi \mathrm{PC}_{i}=\Phi \Phi^{\prime} \mathbf{x}_{i}=\mathbf{x}_{i}
$$

so there is a factor model where factors are PCs from our PCA

\subsubsection{Number of PCs}

Unless the PCs are used for downstream prediction, it is difficult to cross-validate the number of PCs, $K$. People often look at \hl{the proportion of the total variance accounted for by a certain number of PCs}

\begin{align*}
    \text { Total Variance }=\sum_{j=1}^{p} \operatorname{Var}\left(x_{j}\right)&=\sum_{j=1}^{p} \frac{1}{n} \sum_{i=1}^{n} x_{i j}^{2} \\
    \text{Variance explained by the \textit{k}th PC, }\operatorname{Var}\left(P C_{i k}\right)&=\frac{1}{n} \sum_{i=1}^{n} PC_{i k}^{2} \\
    \text{Proportion of variance explained by \textit{K} PCs, }&=\frac{\sum_{k=1}^{K} \frac{1}{n} \sum_{i=1}^{n} P C_{i k}^{2}}{\sum_{j=1}^{p} \frac{1}{n} \sum_{i=1}^{n} x_{i j}^{2}}
\end{align*}

\newpage
\section{Experiments and Controls}

The aim of experiments is to find causal relationships between variables. This is much harder than just prediction. You need to make \textbf{"what if ..."} (counterfactual) predictions, and we will discuss ways of doing that.

\subsection{Potential Outcomes Framework}

Suppose the treatment $d$ is binary, i.e.

$$
d= \begin{cases}1 & \text { Treatment } \\ 0 & \text { No treatment }\end{cases}
$$

The outcome $y$ would depend on the treatment:

$$
y= \begin{cases}y(1) & \text { Treatment, } d=1 \\ y(0) & \text { No treatment. } d=0\end{cases}
$$

Meaning subject $i$ has the potential outcomes $\{y_i(1), y_i(0)\}$ and a treatment effect of $y_i(1) - y_i(0)$. The \hl{average treatment effect} (ATE) takes the expectation across subjects (i.e. the average)

$$
ATE \equiv \mathbb{E}[y(1)-y(0)]
$$

When we observe subject $i$'s outcome, they will either be treated or not: $y_i = y_i(1)$ or $y_i = y_i(0)$, but never both. We need an unobserved counterfactual:

\begin{itemize}
    \item $y_i(0)$ if $d_i = 1$: can be interpreted as the outcome of the treated subject $i$ if they had not been treated.
    \item $y_i(1)$ if $d_i = 0$: can be interpreted as the outcome of the non-treated subject $i$ had they been treated.
\end{itemize}

\begin{note}
    We can never get individual counterfactuals, but we can find the average counterfactual
\end{note}

 For example, if $\mathbb{E}[y(1) \mid d=1]=\mathbb{E}[y(1)]$ and $\mathbb{E}[y(0) \mid d=0]=\mathbb{E}[y(0)]$, straightforward to compute the desired ATE as
\begin{align*}
    A T E & \equiv \mathbb{E}[y(1)-y(0)] \\
& =\mathbb{E}[y(1)]-\mathbb{E}[y(0)] \\
& =\mathbb{E}[y(1) \mid d=1]-\mathbb{E}[y(0) \mid d=0] \\
& =\underbrace{\mathbb{E}[y \mid d=1]}_{\text {Observed }}-\underbrace{\mathbb{E}[y \mid d=0]}_{\text {Observed }}
\end{align*}

Our initial assumptions, $\mathbb{E}[y(1) \mid d=1]=\mathbb{E}[y(1)]$ and $\mathbb{E}[y(0) \mid d=0]=\mathbb{E}[y(0)]$ imply random treatment assignment


\subsection{Randomised Controlled Trials}

Randomised Controlled Trials (RCTs) are the golden standard for evaluating treatment effects. Very simple premise. If you want to know what the treatment $d$ does to an outcome $y$, test out $d$ on a subset of the sample (say 50\%) a.k.a the treatment group, and compare the outcomes to the rest of the same, the control group.

\hl{Random treatment assignment} means that $d$ is independent of the rotational outcomes $y(1)$ and $y(0)$, which gives us:
\begin{align*}
    A T E & \equiv \mathbb{E}[y(1)-y(0)] \\
& =\mathbb{E}[y(1)]-\mathbb{E}[y(0)] \\
& =\mathbb{E}[y(1) \mid d=1]-\mathbb{E}[y(0) \mid d=0] \\
& =\underbrace{\mathbb{E}[y \mid d=1]}_{\text {Observed }}-\underbrace{\mathbb{E}[y \mid d=0]}_{\text {Observed }}
\end{align*}

\subsubsection{Estimating ATE in RCT}

It is easy to estimate ATE in RCT due to the random treatment assignment.

\begin{procedure}
    Consider an RCT with random binary treatment assignment and outcomes: $\{d_i,y_i\}_{i=1}^n$ where $\sum_{i=1}^n d_i = n_1$ (number of treated subjects) and $\sum_{i=1}^n (1-d_i) = n-n_1 = n_0$. The total sample is split into two, treated $n_1$ and non-treated $n_0$. Total subjects is $n$.

    Because of our random treatment assignment, we know that the difference in average outcomes between the treatment and control groups is \hl{caused} by the treatment. Therefore, if $\Bar{y}_1\equiv \frac{1}{n_1}\sum_{i=1}^n d_iy_i$ and $\Bar{y}_0 \equiv \frac{1}{n_0}\sum_{i=1}^n(1-d_i)y_i$, then we know that the estimated ATE will be:
    \begin{equation}
        \widehat{ATE} = \Bar{y}_1 - \Bar{y}_0
    \end{equation}
    and will be a very good estimator for ATE (consistent under the law of large numbers). We can also find the standard errors very easily if the observations are independent:

    \begin{equation*}
        SE(\widehat{ATE}) = \sqrt{\dfrac{1}{n_0}\widehat{\mathbb{V}ar}(y_i|d_i=0) + \dfrac{1}{n_1}\widehat{\mathbb{V}ar}(y_i|d_i=1)}
    \end{equation*}
\end{procedure}

\begin{note}
    The standard central limit theorem implies asymptotic normality.
\end{note}

\subsubsection{Near-experimental designs}
RCTs are often unfeasible and impractical. In these cases, we look for near-experimental variation in observational data to recover treatment effects. We need to include many more assumptions, but we will ignore this. 

You have to find groups of observations that may act as treatment and control groups, even if the treatment assignment is not explicitly random. Examples:
\begin{itemize}
    \item Difference-in-Difference analysis
    \item Regression discontinuity analysis
    \item Instrumental variable analysis
\end{itemize}

\subsection{Conditional Ignorability}

Now we will focus on the use of machine learning to facilitate causal analysis in conditional ignorability and LTE cases.

When we don't have experimental data we have to look at observational data. Instead of controlling for treatment, you are stuck observing what happened when treatment varied in an uncontrolled manner. In these sorts of studies (observation studies) \hl{estimation of the counterfactuals depend on the assumption of conditional ignorability}.

\paragraph{Confounding co-influences:} We must control for all confounding co-influences on treatment $d$ and outcome $y$. These confounding co-influences (or confounders) are variables that simultaneously affect $d$ and $y$. There are always lots of confounders in observational studies, meaning we must do high-dimensional controlling which is where machine learning comes in.

\begin{procedure}
    \hspace{1in}
    Data is $\{d_i, \mathbf{x}_i, y_i\}_{i=1}^n$ where potential outcomes means $y_i = y(d_i)$. If we had an RCT, random treatment assignment \textit{implies} that $d_i$ is independent of the potential outcomes $y(d_i)$ i.e.
    \begin{equation*}
        \{y_i(1), y_i(0)\} \ind d_i
    \end{equation*}

    However, if data is observational, $d_i$ is not independent of potential outcomes $y(d_i)$.
    \begin{example}
        University attendance, $d$, is affected by academic ability $x$, but so are potential outcomes $y(d_i)$. In this example, our independence condition does not hold.
    \end{example}

    If we control (= hold fixed) confounders, we have \hl{conditional independence and therefore conditional ignorability}:
    \begin{equation*}
        \{y_i(1), y_i(0)\} \ind d_i |\mathbf{x}_i
    \end{equation*}

    You can see that conditional ignorability hinges on $\mathbf{x}_i$. In our example, $\mathbf{x}_i$ must include academic ability or at least a good proxy for it.
\end{procedure}

\subsection{Linear Treatment Effect Model}

The linear treatment effects (LTE) model is:

\begin{align*}
    y &= \gamma d + \mathbf{x^\prime}\beta + \epsilon, \quad \E[\epsilon|d,\mathbf{x}] = 0 \\
    d &= \mathbf{x^\prime}\tau + \upsilon, \quad \E[\upsilon|\mathbf{x}] = 0
\end{align*}

The first equation looks like normal regression for prediction. \textbf{But} our specification of the conditional error is new. $\E[\epsilon|d,\mathbf{x}] = 0$, allows interpretation of $\gamma$ as a causal effect of $d$ on $y$. 

the second equation is saying that $\mathbf{x}$ contains \textbf{all} variables affecting $d$ and $y$.

\begin{note}
    The LTE model is a structural model meaning it specifies the relation between unobserved $\epsilon$ and observed $d,\mathbf{x}$; unlike a reduced form model which only specifies the relationship between observable variables.

    This distinction is core to the distinction between prediction and causal inference.
\end{note}

\begin{procedure}
    \hspace{1in}
    Suppose conditional ignorability (on $\mathbf{x}$) ignorability holds, i.e. $\E[\epsilon|d,\mathbf{x}] = 0$. We partition $\mathbf{x} = \{\mathbf{z,w}\}$ and assume $\E[\epsilon|d,\mathbf{w}]\neq 0$. We observe $\mathbf{z}$, but not $\mathbf{w}$.

    Suppose you want to predict $y$ form $(d,\mathbf{z})$. We use the conditional (on $d,\mathbf{z}$) mean function of $y$: $\E(y|d,\mathbf{z})$.

    We can estimate $\E(y|d,\mathbf{z})$ by OLS on:

    \begin{equation}
        \label{LTE OLS}
        y = \alpha d + \eta \mathbf{w} + u
    \end{equation}

    where $u \equiv y - \alpha d - \eta \mathbf{w}$.
\end{procedure}

As long as the average relationship between $y, d, \mathbf{z}$ i.e. $\E(y|d,\mathbf{z})$ is stable, the prediction model works well.

\begin{note}
    \textbf{But} $\hat{\alpha}$ is a poor estimate of the average treatment effect = $\gamma$.
\end{note}

\begin{shaded}
    \begin{itemize}
        \item \textbf{Prediction}: we are not concerned with $\E[\epsilon|d, \mathbf{w}]=0$ so we use the reduced form models
        \item \textbf{Causal Inference}: We need $\E[\epsilon|d,\mathbf{x}]=0$ as it guards against OVB, so we use structural models. 
    \end{itemize}
\end{shaded}

\subsubsection{Fitting an LTE model}

\begin{procedure}
    Recall we have the LTE model:

    \begin{align*}
    y &= \gamma d + \mathbf{x^\prime}\beta + \epsilon, \quad \E[\epsilon|d,\mathbf{x}] = 0 \\
    d &= \mathbf{x^\prime}\tau + \upsilon, \quad \E[\upsilon|\mathbf{x}] = 0
\end{align*}

Suppose you have data $(d_i, \mathbf{x}_i, y_i)_{i=1}^n$ where $\mathbf{x}$ is a p-vector. \hl{If $n\gg p$, we don't need to worry about the second equation. We can just estimate the first by OLS and $\hat{\gamma}$ is a good estimator of $\gamma$}

\begin{note}
    To justify $\E[\epsilon|d,\mathbf{x}]=0$ we need $p$ to be large. If $p>n$ you can't estimate the first equation by OLS and need to select regressors. We can do this by using a \textbf{lasso.}
\end{note}
\end{procedure}

\subsection{LTE Lasso}

When we have many confounders, especially when $p>n$, you need to model treatment assignment in the LTE model (include the second equation). We consider the lasso and cross-validation method.

\begin{shaded}
Difference between lasso for prediction and LTE:
\begin{itemize}
    \item \textbf{Prediction lasso}: Get a path of the candidate model by varying the penalty, and we pick the weight which gives rise to the model with the best cross-validated out-of-sample predictive performance.
    \begin{itemize}
        \item This works well to get a model that predicts new data from the \hl{same joint distribution} of $(\mathbf{x}, y)$ as the old data.
        \item In our treatment effects, the joint distribution changes when $d$ changes so it does not work well. The effect of $d$ on $y$ when $d$ moves independently of $x$, i.e. a \hl{new joint distribution $(d, \mathbf{x}, y)$ when $d$ changes}.
    \end{itemize}
    \item \textbf{LTE lasso}: removes the effect of the confounders that are correlated with $d$ from treatment effect and estimates $\hat{\gamma}$. It can handle large sets of confounders i.e. when $p$ is large.
\end{itemize}
\end{shaded}

\begin{procedure}
    We have the LTE model:

    \begin{align*}
    y &= \gamma d + \mathbf{x^\prime}\beta + \epsilon, \quad \E[\epsilon|d,\mathbf{x}] = 0 \\
    d &= \mathbf{x^\prime}\tau + \upsilon, \quad \E[\upsilon|\mathbf{x}] = 0
    \end{align*}

    \begin{enumerate}
        \item We first model the treatment assignment $\E[d|\mathbf{x}]$
        \item We then use the fitted values $\hat{d} = \mathbf{x^\prime}\hat{\tau}$ as additional controls in the response equation (first one).
    \end{enumerate}
\end{procedure}

The fitted values $\hat{d}$ in the 2nd stage control for the effect of $\mathbf{x}$ on treatment assignment and isolates random variation in $d$ coming from $\upsilon$
\begin{note}
    The random variation in $d$ coming from $\upsilon$ can be thought of as little experiments in $d$, \hl{so we can use the fitted values $\hat{d}$ to create a near-experimental design}.
\end{note}

Everything hinges on having fitted values $\hat{d}$ that captures the relevant influences on $\mathbf{x}$ on $d$, but the lasso can be good for this.

\begin{algo}
\hspace{1in}
    \begin{enumerate}
        \item Cross-validate a lasso to estimate $\E[d|\mathbf{x}] = \mathbf{x^\prime}\tau$ and collect the fitted values $\hat{d} = \mathbf{x^\prime}\hat{\tau}$
        \item Cross validate a lasso to estimate:
        \begin{equation}
            \label{2nd step LTE lasso}
            \E[y|\mathbf{x}, d] = \vartheta \hat{d} + \gamma d + \mathbf{x^\prime}\beta
        \end{equation}
        with no penalty on $\hat{d}$, because we need to keep control to maintain the conditional ignorability and if we were to penalise $\hat{d}$ the coefficient would go to 0 and we would lose it.
        \item Then, our $\hat{\gamma}$ is our estimate of the treatment effect of $d$ on $y$.
    \end{enumerate}
\end{algo}

\subsubsection{Why does the LTE lasso work?}

When we combine the two LTE model equations:

\begin{equation*}
    \E[y|\mathbf{x},d] = \gamma d + \mathbf{x^\prime}\beta = \gamma\upsilon + \mathbf{x^\prime}(\gamma\tau + \beta) = \gamma\upsilon + \mathbf{x^\prime}\tilde{\beta}
\end{equation*}

Controlling for our fitted values $\hat{d}$ unpenalised in the second step means that we can estimate the effect of the residuals $\hat{\upsilon}$ which is independent of cofounder influence in $\hat{d}$.

\begin{note}
    The special thing about LTE lasso is that it handles large $p$ and automatically picks out the relevant predictors of treatment assignment.
\end{note}

\begin{note}
    the LTE lasso gives point estimates of the treatment effect but \hl{does not have good inferential properties i.e. standard errors}
\end{note}

\appendix

\clearpage

\section{Dimension Reduction Methods}

\label{appendix: dimension reduction}

With our simple models, we look at controlling our variance by either using a subset of the original variables, or we shrink the coefficients towards zero i.e. using predictors $\mathbf{x_1, x_2, \ldots, x_p}$. \hl{Dimension reduction methods are a class of approach that transforms the predictors and then fits a least squares model using the transformed variables}.

\begin{definition}
    Let $Z_1, Z_2, \ldots, Z_M$ represent $M<p$ \textit{linear combinations} of our original $p$ predictors. 

    \begin{equation*}
        Z_m = \sum_{j=1}^p \phi_{jm}\mathbf{x_j}
    \end{equation*}

    for some constants $\phi_{1m}, \phi_{2m}, \ldots, \phi_{pm}$ for $m= 1, \ldots, M$. We can then fit the linear regression model:

    \begin{equation}
        \label{dim reduction lin reg}
        y_i = \theta_0 + \sum_{m=1}^M \theta_m z_{im} + \epsilon_i, \qquad i = 1, \ldots, n
    \end{equation}
    using least squares. All dimension reduction methods work in two steps:

    \begin{enumerate}
        \item Get the transformed predictors $Z_1, Z_2, \ldots, Z_M$.
        \item Fit the model with these $M$ predictors. 
    \end{enumerate}
    However, the choice of our new predictors (or equivalently the $\phi_{jm}$'s) can be achieved in different ways.
\end{definition}

\begin{note}
    Notice:

    \begin{equation*}
        \sum_{m=1}^M \theta_m z_{im} = \sum_{m=1}^M \theta_m \sum_{j=1}^p \phi_{jm}x_{ij} = \sum_{j=1}^p \sum_{m=1}^M \theta_m \phi_{jm} x_{ij} = \sum_{j=1}^p \beta_j x_{ij},
    \end{equation*}
    where
    \begin{equation*}
        \beta_j = \sum_{m=1}^M \theta_m \phi_{jm}.
    \end{equation*}
\end{note}

\subsection{Principal Component Analysis (PCA)}

PCA is a technique for reducing the dimension of an $n \times p$ data matrix $\mathbf{x}$. The \textit{first principal component} direction of the data is that along which the observations \textbf{vary the most} (more variance = more information = better regressor).

pg. 253 ISLR












\end{document}